# INFO1111 Exam Textbook

# Chapter 1  
INFO1111 — Computing 1A  
Week 01: Professionalism

## 1  The Value of Continuous Learning

Jose M. Aguilar reminds us that “Your knowledge today does not have much value beyond a couple of years. Your value is what you can learn and how easily you can adapt to the changes this profession brings so often.” In computing, technologies, practices and even whole job categories appear and disappear in the span of a degree. Professional relevance therefore hinges on the capacity to learn rapidly and repeatedly throughout one’s career.

## 2  A Domino Puzzle and the Habit of Problem-Solving

Consider an almost-filled chessboard:

* Scenario A: remove no squares from a \(4 \times 4\) or \(8 \times 8\) board.  
  Total squares: 64 (32 dark, 32 light).  
  Thirty-two dominoes, each covering one dark and one light square, can tile the board.

* Scenario B: remove one dark and one light square from opposite corners of an \(8 \times 8\) board.  
  Total squares: 62 (31 dominoes required).  
  Colour count: 31 dark, 31 light.  
  The board is still tileable in principle.

* Scenario C: remove two opposite corners of identical colour (both dark).  
  Total squares: 62, but the distribution becomes 32 dark and 30 light.  
  Because every domino must cover exactly one dark and one light square, perfect tiling is impossible.

The exercise highlights a core professional skill: analysing constraints before jumping to implementation. Clever reasoning, not merely coding effort, often determines success.

## 3  What Computing Graduates Actually Do

Students arrive motivated by games, coding enthusiasm, entrepreneurial dreams or a desire to improve lives. Once employed, graduates discover that writing code occupies only a fraction of their week. Time is also spent:

* clarifying requirements with stakeholders,  
* designing systems and data models,  
* testing and debugging,  
* analysing business processes,  
* providing production support, and  
* communicating results to technical and non-technical colleagues.

Job advertisements illustrate this breadth. Searching Australian employment sites for “IT”, “software”, or “programming” yields titles such as:

* Java Developer / Full-Stack Developer  
* Cloud Developer  
* Business Systems & Database Administrator  
* IT Support Officer

Each description lists a mix of technical, analytic and interpersonal expectations.

## 4  Essential Technical Activities

Typical daily tasks include:

* coding, testing and debugging,  
* system and interface design,  
* rapid prototyping,  
* requirements elicitation,  
* data and business analysis,  
* hardware integration and operational support.

### 4.1  Tech Stacks

A tech stack is an inter-related set of technologies combined to deliver a solution.  
Example (canonical web stack): **LAMP** — Linux, Apache, MySQL, PHP/Perl/Python.  
Stacks may be project-specific or adopted organisation-wide; fluency with at least one full stack is expected of modern professionals.

### 4.2  Skills Requested in a Typical Advertisement

Role: Java Developer  
Required skills:  

* Java, JavaScript, CSS, HTML5  
* End-to-end (E2E) development experience  
* Hibernate and RESTful services  
* Agile/SCRUM practice  
* Eclipse, JIRA, GitHub  
* Strong communication in English  
* Bachelor of Computer Science and ~2 years experience

## 5  Technological Change and Uncertainty

### 5.1  Roles Lost and Gained

Disappearing in 2022: Flash Developer, Windows XP Administrator, Fortran Programmer.  
Common in 2024 yet rare in 2000: User Experience Designer, Cloud Developer, Data Miner, Internet-of-Things Engineer.

### 5.2  A Rapid Timeline

2004 Facebook | 2007 iPhone & Netflix Streaming | 2010 iPad | 2016 TikTok | 2022 ChatGPT  
Such introductions repeatedly redefine required skill sets.

### 5.3  Faulty Predictions

History is littered with confident but wrong forecasts:

* “I think there is a world market for maybe five computers.” — Thomas Watson, 1943  
* “There is no reason anyone would want a computer in their home.” — Ken Olsen, 1977  
* “Spam will be a thing of the past in two years’ time.” — Bill Gates, 2004  

The lesson: plan to adapt, not to predict perfectly.

## 6  Lifelong Self-Learning

If a graduate stops learning, professional relevance decays quickly. To cultivate intentional learning, this unit introduces a weekly self-learning exercise. The first theme is the “Drunkard’s Walk”, a metaphor for unpredictable but persistent exploration. Students research the concept independently, then discuss insights in the following lecture.

## 7  Learning Structure in Computing 1A

### 7.1  Three Learning Categories

1. Knowledge  
2. Skills  
3. Self-Learning

Each category has a **Foundation** task and an **Advanced** task.

### 7.2  Progression Without Marks

Attempts are rated **Poor / OK / Strong**.  
A **Strong** rating in a Foundation task unlocks the corresponding Advanced task; otherwise the student resubmits the Foundation task. Final grades depend on the highest level achieved in each category rather than on accumulated marks.

Minimum for a Pass (50): OK in Knowledge and Skills foundations  
Higher grades require stronger or advanced performance across categories.

### 7.3  Late Penalties

Because no marks are awarded per task, lateness incurs a penalty of 0.5 mark per calendar day, deducted from the final unit mark. Special consideration is available when justified.

## 8  Independent Learning Activity: Markdown

As an initial test of autonomous learning, students spend no more than two to three hours teaching themselves **Markdown**, a lightweight document-formatting language widely used in online publishing and technical documentation. Mastery is self-assessed; copying achieves nothing.

## 9  Professional Study Habits

Metacognition — Monitor progress against published learning outcomes and relate each assessment to those goals.  
Time Management — Track due dates, start early, submit early.  
Networking — Form study communities, know your tutors and coordinator, and communicate promptly when difficulties arise.  
Enjoyment — Curiosity and enthusiasm sustain long-term professional growth.

---

Professionalism in computing is less about what you currently know and more about how effectively you can learn, reason, and collaborate. The discipline evolves incessantly; embracing continuous self-development is the surest path to a resilient and rewarding career.

# Chapter 2  
INFO1111 — Computing 1A  
Week 02: Professionalism, Teams, and the Command-Line

## 1 Professionalism Revisited

Edsger W. Dijkstra’s observation that “Computer Science is no more about computers than astronomy is about telescopes” reminds us that technical artefacts are merely instruments. Felipe Jara updates the analogy: software development is no more about coding than running a restaurant is about ingredients. True professionalism therefore extends far beyond syntax and algorithms.

### 1.1 The Restaurant Analogy

Running a successful restaurant requires much more than flour, rice, or beef; it involves recipes, equipment, staff management, finance, marketing, customer service, and ambience. A parallel set of layers applies to computing:

* Technology – Java, Python, Linux  
* Frameworks – Django, Angular  
* Tools – Eclipse, JUnit, Git  
* Techniques – Agile, DevOps, Pair Programming  
* Skills – Coding ability, teamwork, communication  
* Professional Context – Ethics, business processes, stakeholder needs  

Technical capability is fundamental, but—like flour without a recipe—it is insufficient on its own. Professionalism integrates all layers into a coherent practice.

### 1.2 The Skills Framework for the Information Age (SFIA)

The **Skills Framework for the Information Age (SFIA)** is an international reference model that catalogues and levels the capabilities demanded of digital professionals. It provides:

* A common vocabulary for skills such as systems design, penetration testing, or service management.  
* A scale of responsibility levels from entry-level practitioner to strategic leader.  

SFIA underlines that career progression depends on broadening responsibility and judgement, not just accumulating technical knowledge.

## 2 Working in Teams

Human–computer interaction dominates software engineering, yet human–human interaction is equally critical. Effective teams magnify individual strengths and mitigate weaknesses.

### 2.1 What Is a Team?

BusinessDictionary:  
“A group of people with a full set of complementary skills required to complete a task, job, or project.”

Hackman et al.:  
“An intact social system, complete with boundaries, interdependence for some shared purpose, and differentiated member roles.”

Wikipedia:  
“A group of individuals working together to achieve a goal. Teams normally have members with complementary skills and generate synergy through coordinated effort.”

### 2.2 Team Structures in Computing

1. Team types  
   • Development • Operations • Quality Assurance • Support • Security  
2. Team topologies  
   • Stream-aligned • Enabling • Platform  
3. Role categories  
   • Analyst • Programmer • Architect • Tester  
4. Modes of working  
   • Solo programming • Pair programming • Mob programming

(For an overview of Agile team structures see: https://relevant.software/blog/what-agile-software-development-team-structure-looks-like/.)

### 2.3 Persistent Myths About Teams

* Harmonious teams automatically outperform teams with conflict.  
  – Research shows that constructive disagreement can raise performance; “grumpy orchestras” often play better than uniformly happy ones.  
* Leadership style alone determines dynamics.  
* Bigger teams always perform better.  
* Stable membership guarantees improvement.  
  – 73 % of recorded airline incidents occurred on a crew’s first day flying together, but long-standing teams can also stagnate.

### 2.4 Hackman’s Model of Team Effectiveness

Successful groups:

1. Satisfy internal and external clients.  
2. Build competence for future tasks.  
3. Provide members with meaning and satisfaction.

Five enabling factors:

1. A real team (shared task, clear boundaries, stable membership)  
2. Compelling direction (clear, challenging, achievable goals)  
3. Enabling structure (suitable size, balanced skills, sound work processes)  
4. Supportive context (rewards, information, training resources)  
5. Expert coaching (timely mentoring and feedback)

### 2.5 Teams in Professional Software Development

Professional work is:

* Multi-disciplinary (business analysts, UX designers, developers, testers)  
* Multi-faceted (analysis, architecture, coding, verification)  
* Collaborative (paired or ensemble programming, code reviews)  

Processes range from plan-and-document methodologies to Agile approaches such as **SCRUM** and **Extreme Programming (XP)**. Pair programming, for example, seeks rapid knowledge transfer and continuous code review.

### 2.6 Teams in Student Projects

Student teams differ from commercial teams because they:

* Lack fully shared fate—grades substitute for business impact.  
* Face limited real-world consequences.  
* Operate on divergent personal schedules.  

Empirical observations of student groups show:

Successful teams  
• Equal contributions  
• Open discussion  
• Mutual support  

Common problems  
• Logistics (meeting times, communication)  
• Poor task allocation  
• Weak coordination  
• Inconsistent commitment  

Strategies for improvement  
• Draft a team constitution early.  
• Discuss cultural differences and expectations openly.  
• Confront negative behaviours such as freeloading or dominance quickly and constructively.

### 2.7 Diversity and Implicit Bias

Diversity is “the inclusion of different types of people in a group or organisation” (Merriam-Webster). All teams exhibit diversity in background, experience, and perspective, and every member carries implicit biases. Awareness initiatives such as Project Implicit (https://www.projectimplicit.net/) highlight unconscious attitudes that can influence decisions. Effective teams recognise these biases, foster inclusive dialogue, and leverage varied viewpoints to improve outcomes.

## 3 Why Software Projects Fail

Industry surveys reveal sobering statistics:

* ~30 % succeed (deliver the right functionality on time and within budget).  
* ~50 % deliver poor outcomes (reduced scope, delays, cost overruns).  
* ~20 % are abandoned entirely.  

During 2020 alone, failed projects wasted an estimated USD 300 billion; late or over-budget projects consumed another USD 750 billion. Few other engineering disciplines tolerate comparable failure rates. Complex, rapidly changing requirements and the intangible nature of software contribute to this difficulty and reinforce the need for disciplined teamwork and professionalism.

## 4 Command-Line Interfaces and Scripting

The second half of Week 2 shifts from organisational skills to practical tooling. Understanding command-line environments remains foundational even in an era dominated by graphical user interfaces (GUIs).

### 4.1 A Layered View of System Interaction

```
Applications (GUI)
Graphical Interface
Command-Line Interface (CLI)
Operating System
Hardware
```

Each layer abstracts the one beneath it. The CLI exposes operating-system services directly yet remains independent of specific application interfaces.

### 4.2 Common Shells

* **bash** (Bourne Again SHell) – default on most Linux and macOS systems.  
* **zsh** (“Z shell”) – offers enhanced completion and customization.  
* **PowerShell** – task automation and configuration management for Windows, now cross-platform.

### 4.3 Essential Navigation and File Commands

```
pwd        # print working directory
cd …       # change directory
mkdir …    # create directory
rmdir …    # remove empty directory
rm …       # delete file (del … on Windows)
echo …     # display a line of text
cat …      # concatenate / display file contents
```

Mastery of a small core set accelerates everyday tasks and supports scripting.

### 4.4 Automating Tasks with Scripts

A **script** is a plain-text file containing a sequence of CLI commands executed automatically. Scripts are used to:

* Reproduce builds or deployments reliably.  
* Batch-process data sets.  
* Configure development environments.

(An in-class demonstration illustrates chaining of the above commands into a reusable bash script.)

## 5 Independent Learning Theme: Big-O Notation

Following last week’s exploration of the Drunkard’s Walk, the Week 2 self-learning topic is **Big-O notation**—the mathematical language for describing algorithmic growth rates. Students research this concept independently and share findings in tutorial.

---

Professionalism blends technical execution, teamwork, ethical awareness, and effective tooling. Whether negotiating project scope, coordinating a diverse team, or automating a build with a shell script, the modern computing professional must operate confidently across multiple layers of responsibility.

# Chapter 3  
INFO1111 — Computing 1A  
Week 03: Communication and LaTeX

## 1 Setting the Scene

Professional software practice rests on two complementary pillars:

1. Technical competence (algorithms, tools, notation)  
2. The ability to convey ideas so they are understood and acted upon

Week 03 concentrates on the second pillar—communication—while introducing LaTeX, the typesetting language that forces writers to think clearly about structure and content. Before exploring these themes, we briefly revisit key notions from Week 02.

## 2 Week 02 in One Minute

The previous chapter examined professionalism, teamwork, and the command line. Two concrete questions framed the discussion:

* How does a pipeline such as  
  `cat names.txt | grep "David" | head -4 | sort`  
  transform a data set?
* How do algorithmic costs scale?  
  – Averaging *N* numbers grows linearly, *O(n)*.  
  – Binary search grows logarithmically, *O(log n)*.

Both topics resurface repeatedly: command-line fluency accelerates everyday work, and complexity analysis underpins performance discussions.

## 3 Independent Learning Themes

### 3.1 Complexity Revisited

Students independently compared *O(1)*, *O(log n)*, *O(n)*, and *O(n²)* algorithms using external resources such as developerinsider.co. The objective was to solidify an intuition for growth rates before they appear in later coding tasks.

### 3.2 Week 03 Focus: Regular Expressions

The self-learning challenge for this week introduces regular expressions (often abbreviated **RegExp**). One illustrative task investigates the stream editor `sed`:

```
sed "s|\(^T[io]m\)\W|\1my |g" names.txt > newnames.txt
```

Students dissect the pattern to determine how it:

1. Captures any line starting with `Tim` or `Tom`.
2. Retains the captured text (`\1`) and inserts the characters `my`.
3. Writes the modified content to `newnames.txt`.

Understanding such patterns is essential when automating text processing tasks that arise during data cleaning, log analysis, or build scripting.

## 4 Foundations of Professional Communication

### 4.1 What Is Communication?

Merriam-Webster defines communication as “a process by which information is exchanged between individuals through a common system of symbols, signs, or behaviour.” Two phrases deserve emphasis:

* Information is **exchanged**—both parties must participate.  
* A **common** system is employed—words, diagrams, gestures, or code.

Beyond mere exchange, professional communication is judged by effectiveness: did the receiver interpret the message as intended?

### 4.2 Tone and Appropriateness

Consider three emails requesting help with non-responsive team-mates:

1. Informal: “Hiya, I wanna get help bc my team wont work. IDK. ¯\\_(ツ)_/¯ thx bigD”  
2. Polite but brief:  
   “Hi David,  
   I’m having problems with my group members who just aren’t responding to any emails over the last few days.  
   Can you suggest how I should handle this?  
   Thanks, Dan”
3. Formal, fully specified:  
   “Dear Professor Lowe,  
   Over the last week I have sent emails to my INFO1111 CC34-2 group members about the assignment on Fri, Sat twice, and then this Sunday morning. I haven’t had a single response. This is very unfair to me. Would you please email them and bcc me, indicating that this is unacceptable?  
   Kind regards,  
   Daniel Smith (SID 123456789)”

Each variant communicates a different level of respect, clarity, and professionalism. Selecting the correct tone depends on purpose, audience expectations, and desired outcome.

### 4.3 Signs and Semiotics

Slides showing numbered symbols ask, “What do these ‘signs’ communicate, and how do you know?” The exercise illustrates *semiotics*—the study of signs and their interpretation. Effective communicators exploit shared conventions (road signs, emojis, syntax highlighting) to transfer meaning quickly.

### 4.4 Models of Communication

#### 4.4.1 Transmission Model (Claude Shannon)

```
Source  →  Encoder  →  [Channel]  →  Decoder  →  Destination
```

Shannon’s information-theoretic view emphasises the raw transfer of messages and the possibility of noise. A practical computing analogy is a REST call whose JSON payload must survive network interference.

#### 4.4.2 Interaction Model

```
Sender ➜ Encoder ➜ Decoder ➜ Receiver
          ▲                       ▼
          │<────── Feedback ──────│
```

This model highlights feedback loops. Pair programming embodies it: one developer (“driver”) types while the other (“observer”) continuously responds.

#### 4.4.3 Transaction Model

Messages, channels, senders, and receivers are interwoven; multiple receivers can provide simultaneous feedback, modifying the ongoing exchange. Stand-up meetings illustrate this model—dialogue evolves dynamically as each participant contributes.

### 4.5 Responsibility for Clarity

Is the sender or the receiver accountable for successful communication? In practice responsibility is shared: the sender must encode ideas clearly and the receiver must seek clarification when ambiguity remains.

### 4.6 Media, Scale, and Purpose

Different situations demand different media:

* Written text (one-page summary or 50-page report)  
* Oral presentations (10-minute pitch or 3-hour workshop)  
* Interactive sessions (pair debugging in an IDE)

Factors that differentiate forms include:

1. Medium (email, slide deck, live demo)  
2. Scale (length, duration)  
3. Audience type and size (manager vs colleague, one-to-one vs broadcast)  
4. Interaction pattern (one-way, Q&A, conversational)  
5. Purpose (inform, persuade, motivate)

#### 4.6.1 Directions: Text vs Map

A slide lists eight textual walking directions beginning “Start at the New Law Building…”. The next slide shows a corresponding map. Although both contain identical information, the visual representation conveys spatial relationships more intuitively. The lesson: choose the form that best supports your communicative goal.

### 4.7 Communication Tasks in IT Practice

Typical professional scenarios include:

* Defining inputs and outputs for a new code module  
* Writing a consultancy report for senior managers  
* Presenting project plans to client representatives  
* Requesting budget for tools and training  
* Documenting system architecture for future maintenance  
* Teaching colleagues how to use a specialised tool

Each task varies in medium, detail, and audience, yet all require clear objectives.

### 4.8 Setting Goals and Understanding Context

Before communicating, clarify:

1. Knowledge goals: What should the audience *know* afterwards?  
2. Action goals: What should the audience *do*?  
3. Affective goals: What feelings or beliefs should change?

Adapt explanations to the audience’s starting point—what they already know, expect, and value. Avoid unexplained jargon unless the intent is to challenge or surprise.

## 5 LaTeX for Technical Writing

### 5.1 From TeX to LaTeX

TeX is Donald Knuth’s typesetting program; LaTeX layers higher-level macros on top of TeX to simplify document preparation. Authors write plain text sprinkled with markup commands, then **compile** the source to produce output (usually PDF). The approach guarantees identical results across platforms and points in time.

### 5.2 Why LaTeX in INFO1111?

Although industry often relies on word processors, LaTeX remains dominant in academic and scientific writing because it:

* Separates **content and structure** from presentation  
* Handles mathematics, bibliographies, and cross-references elegantly  
* Demonstrates computing concepts such as compilation and version control  
* Is free and platform-independent

### 5.3 A Minimal Example

```latex
\documentclass[12pt]{article}
\usepackage{amsmath}          % mathematics support

\title{First example}
\author{David}
\date{\today}

\begin{document}
\maketitle                    % generate title

\section{First section}
This is the \textbf{first section} in my \LaTeX{} document.
I can include maths directly like $a^2 + b^2 = c^2$,
or refer to a displayed equation~\ref{aaa}.

\begin{equation}
\label{aaa}
\gamma^2 + \theta^2 = \omega^2
\end{equation}

That was so easy.
\end{document}
```

Key observations:

* Preamble commands (`\documentclass`, `\usepackage`) establish global settings.  
* Structural commands (`\section`) impose hierarchy.  
* Mathematical material is delimited by `$ … $` (inline) or an `equation` environment (display).  
* Cross-references use labels (`\label{aaa}`) and references (`\ref{aaa}`).

### 5.4 Compilation Workflow

1. `pdflatex` reads the `.tex` file, producing a PDF and auxiliary file `.aux`.  
2. If bibliography entries are present, `bibtex` processes the `.aux`, generating a `.bbl` file.  
3. Running `pdflatex` again incorporates the bibliography and resolves cross-references.  
4. A final `pdflatex` pass ensures all links are correct.

Diagrams on the slides illustrate the repeated passes required for “wrong” vs “right” reference resolution and for integrating BibTeX output.

### 5.5 Benefits Recap

LaTeX enables:

* Focus on content rather than layout  
* Consistent appearance via class or style files  
* Robust indexing, cross-referencing, and bibliographic support

These advantages mirror software engineering principles such as modularity and separation of concerns.

## 6 Looking Ahead

Week 03 positions communication as a core professional skill and equips students with LaTeX—a practical tool for written expression. Tutorials reinforce both topics, and the self-learning exercise on regular expressions builds further command-line fluency. With teams now established, the stage is set for applying these skills in forthcoming assignments.

# Chapter 4  
INFO1111 — Computing 1A  
Week 04: Tech Stacks, APIs, and Makefiles

## 1 Connecting with Last Week

Week 03 emphasised clear communication and introduced LaTeX as a compilation-based writing tool. You should now be comfortable editing a `.tex` file and invoking `pdflatex` (and, when needed, `bibtex`) from the command line.

A further self-learning exercise explored regular expressions. One illustrative command was

```bash
sed "s|\(^T[io]m\)\W|\1my |g" names.txt > newnames.txt
```

which finds every word beginning with *Tim* or *Tom*, captures it, appends the characters `my`, and writes the updated stream to `newnames.txt`.

## 2 Independent Learning for Week 04

This week’s concept is the **greedy algorithm**—an approach that repeatedly makes the locally optimal choice, hoping to reach a global optimum. Greedy methods succeed for tasks such as building a minimum spanning tree with Kruskal’s algorithm, yet fail for others; for example, choosing the largest-denomination coin at each step does not always yield the minimal number of coins in arbitrary currency systems. Identifying where greediness is sub-optimal sharpens algorithmic judgement.

---

## 3 Technology Stacks

### 3.1 Definition

A **tech stack**—also called a *solutions stack*—is the collection of inter-related technologies used to build and run an application or project. According to Heap Inc.:

> “A tech stack typically consists of programming languages, frameworks, a database, front-end tools, back-end tools, and applications connected via APIs.”

Stacks may be assembled for one specific solution or standardised across an entire organisation.

### 3.2 Typical Layers and Roles

• Operating system  
• Server infrastructure  
 – Data management services  
• Back-end components (core functionality)  
• Front-end components (user interfaces)  
• Communication mechanisms  
• Load balancers  
• Monitoring and logging tools

Each layer fulfils a distinct role yet must integrate seamlessly with its neighbours.

### 3.3 Integrating the Layers

Combining stack layers involves

1. Configuration files (for example, YAML or JSON deployment descriptors)  
2. Code that calls lower-level services through libraries or system calls  
3. Network sockets, REST endpoints, or message queues that move data between processes or machines

Careful orchestration of these mechanisms yields a coherent system rather than a fragile assortment of parts.

### 3.4 Enterprise Perspective

Businesses increasingly maintain **matrixed tech stacks** that aggregate technologies both horizontally (shared services such as authentication) and vertically (domain-specific analytics). Visualisations of such stacks appear in industry surveys (e.g. chiefmartec.com, 2022) and underscore the strategic importance of technology selection.

---

## 4 Application Programming Interfaces (APIs)

### 4.1 What an API Is

An **Application Programming Interface** exposes a set of operations that external code can invoke. APIs decouple provider implementation from consumer usage, encouraging modularity and third-party integration. A well-known example is the Google Maps API.

### 4.2 Layer-by-Layer Interaction

The slides depict a progressive architecture:

1. **Web Browser** → *Your JavaScript code* → Maps rendered inside the page  
2. Browser → **Web Server** → *Your code* → **Database** for local data + calls to Google Maps  
3. Browser → Web Server → **Google API server** (over HTTPS) → Google’s back-end  
   meanwhile, your own back-end services a **Database Management System (DBMS)**

Although diagram details differ, the constant theme is that an API abstracts geographic data services behind documented HTTP endpoints. Your application interacts with these endpoints rather than manipulating raw map tiles.

---

## 5 Automating Builds with Makefiles

### 5.1 Why Build Automation Matters

Constructing an application rarely consists of a single compilation step; source files, libraries, documentation, and tests interact in intricate ways. Manual coordination is error-prone and slow.

### 5.2 Core Concepts

A **makefile** expresses three facts about each build artifact:

1. **Target** – the file to be created or updated  
2. **Dependencies** – files that must exist and be newer than the target  
3. **Process** – shell commands that generate or refresh the target

`make` evaluates timestamps, decides which targets are out of date, and executes only the necessary commands.

### 5.3 Minimal Example

```make
# File: Makefile
test1: test2
	echo "Creating test1"
	touch test1

test2:
	echo "Creating test2"
	touch test2
```

Running `make test1` builds `test2` first (because it is a dependency) and then `test1`.

### 5.4 A LaTeX-Based Project

```make
# Variables
filename = INFO1111_Group_Project_CC99-01
bibfile  = main.bib

# Top-level target
pdf: $(filename).pdf
	@echo
	@echo "PDF files now up to date"

# Rule for producing the PDF
$(filename).pdf: $(filename).tex $(bibfile)
	pdflatex $(filename)
	bibtex   $(filename) || true      # proceed even if no citations
	pdflatex $(filename)
	pdflatex $(filename)
	@echo
	@echo "$(filename) has been updated"

# Convenience target
clean:
	rm -f $(filename).{ps,pdf,log,aux,out,dvi,bbl,blg,toc}
```

This makefile records that the final PDF depends on:

• the primary LaTeX source `$(filename).tex`  
• the bibliography file `main.bib`

If either dependency changes, `make` recompiles the document, runs `bibtex`, and performs the additional LaTeX passes required to resolve cross-references. The `clean` target removes intermediate artefacts, keeping the working directory tidy.

---

## 6 Key Takeaways

1. A tech stack is a layered set of technologies—operating systems, back-end frameworks, databases, front-end tools—selected to work together.  
2. APIs expose functionality across process and organisational boundaries; the Google Maps API illustrates how third-party services integrate into your code base.  
3. Build automation via makefiles encodes dependencies and commands, ensuring consistent, efficient compilation of multi-file projects.  
4. The ongoing self-learning theme introduces greedy algorithms, challenging you to recognise when “locally best” decisions fall short of the global optimum.

With these elements—architectural awareness, integration through APIs, and disciplined build processes—you are better equipped to manage the technical complexity encountered in professional computing projects.

# Chapter 5  
INFO1111 — Computing 1A  
Week 05: Finding Information, Collaboration, and Git

## 1 Connecting with Last Week

Week 04 introduced *makefiles* and asked you to build a target named `setup` that creates the initial `.tex` and `.bib` files for a LaTeX project. The independent learning theme explored **greedy algorithms**, illustrating how a strategy that always chooses the locally optimal item may or may not reach a global optimum.

The new self-learning concept for Week 05 is the **race condition**—an error that arises when two or more concurrent operations attempt to access or modify shared data in an order that leads to unexpected behaviour. Race conditions are especially important in multiprocessor systems, parallel programs, and any cloud service where simultaneous requests interact with the same resources.

---

## 2 Week 05 Overview

This chapter concentrates on three professional skills essential to modern software development:

1. Finding authoritative information and evaluating sources.  
2. Collaborating effectively, both as students and as industry professionals.  
3. Managing artefacts with version control, emphasising Git and GitHub.

---

## 3 Finding Information

### 3.1 Sources and Resources

In academic and professional computing, a *source* is any material from which facts, ideas, or data are drawn. Frequently used resources include:

• The University of Sydney Library  
 ‒ Research portal: https://library.sydney.edu.au/research/index.html?accordion=findinfo  
 ‒ Database index: https://library.sydney.edu.au/databases/  

• Google Scholar — https://scholar.google.com.au/  
• ACM Digital Library — https://dl.acm.org/  
• Numerous curated databases and open web repositories.

### 3.2 Evaluating Sources with the REVIEW Criteria

Before citing a document, apply the following checklist:

R — Relevance  
 Is the content directly applicable to your problem or report?

E — Expertise of author  
 What are the author’s qualifications? Are they writing within their domain and cited by peers?

V — Viewpoint or bias  
 Does the author or sponsoring organisation stand to gain from a particular conclusion?

I — Intended audience  
 Is the writing aimed at researchers, practitioners, or the general public?

E — Evidence  
 Are claims supported by primary data, secondary analysis, expert opinion, or mere anecdote? Is the work peer-reviewed?

W — When published  
 Has the field advanced significantly since publication?

A quotation from an online discussion illustrates how these factors combine:

> “Without knowing Git, it is very hard to share your work with other team members.”  
> — Answer on Quora, *Should a data scientist learn Git?* (URL accessed March 2018)

While the statement is persuasive, its authority depends on the writer’s expertise and on corroborating evidence from peer-reviewed literature or documented industrial practice.

### 3.3 Referencing

Referencing serves two purposes:

1. Acknowledging the original creator of ideas, text, data, or code.  
2. Guiding readers to the cited material so they can verify your interpretation.

Direct quotations must appear in quotation marks with a full citation. Paraphrased ideas, reused diagrams, and even your own previously submitted work also require proper attribution. Accurate referencing is central to **academic integrity**.

### 3.4 Academic Integrity: Permitted and Prohibited Actions

Permitted:

• Researching, analysing, critiquing, and comparing the published work of others, provided every use is cited.

Prohibited:

• Copying text or code word-for-word without citation.  
• Paraphrasing another author’s ideas as your own.  
• Recycling your past assignments without disclosure.  
• Fabricating data or results.  
• Submitting or facilitating somebody else’s work under your name.

Always distinguish clearly between your original contributions and the material you have sourced.

---

## 4 Collaboration

### 4.1 Defining Collaboration

Merriam-Webster, Cambridge, and Wikipedia converge on the notion that collaboration is “the process of two or more people working together to complete a task or achieve a goal.” For students, collaboration appears in group projects, peer-review exercises, and shared research. In industry, the stakes rise: schedules tighten, errors cost money, and every change must be traceable.

### 4.2 Core Skills

Successful collaboration combines:

• Teamwork — planning, compromise, and collective responsibility.  
• Communication — clear, timely, audience-appropriate explanations.  
• Technical techniques — shared design patterns, coding standards, and testing practices.  
• Tools — version control, issue trackers, continuous-integration servers.  
• Frameworks — project-management methodologies such as Agile, Scrum, Kanban, Lean, and Waterfall.

### 4.3 Industry Requirements

Professional environments introduce additional constraints:

• Security: protecting intellectual property and personal data.  
• Traceability: knowing *who* changed *what* and *why*.  
• Accountability: assigning responsibility for decisions and defects.  
• Version control: maintaining coherent histories of artefacts under continuous change.

---

## 5 Version Control

### 5.1 Why Version Control?

Imagine a team editing `report_FINAL_v17_REAL_FINAL.docx`. Without a systematic approach, parallel edits collide, bugs creep in unnoticed, and the ability to revert to a working state disappears. Version control addresses:

• Versioning — recovering any previous snapshot when a defect appears.  
• History — attributing each change to an individual commit with an explanatory message.  
• Parallel development — allowing multiple branches to diverge and merge safely.  
• Security and backup — distributing multiple copies of the repository.

### 5.2 Evolution of Version Control Systems

1970 s Conceptual precursors (e.g. SCCS)  
1982 RCS (Revision Control System)  
1986 CVS (Concurrent Versions System)  
2004 Subversion (SVN)  
2005 Git and Mercurial

Front-ends such as Bitbucket and GitHub simplify hosting and collaboration. Google Trends data over the past decade reveal Git’s dominance relative to older systems.

---

## 6 Git: Distributed Version Control in Practice

### 6.1 Key Concepts

Git stores a *snapshot* of your project every time you make a **commit**. The workflow is:

1. Edit files (working directory).  
2. **Stage** selected changes with `git add`.  
3. Record a snapshot with `git commit -m "Message"`.  
4. Traverse history: `git log`, `git diff`, `git checkout <commit>`.  
5. Create a **branch** to explore an alternative line of development.  
6. **Merge** branches, resolving conflicts if necessary.  
7. **Push** local commits to a remote repository; **pull** others’ work to your machine.

Because Git is distributed, every clone contains a full copy of the entire history; a central server is convenient but not required.

### 6.2 Remote Storage Providers

Common hosting services include:

• GitHub  
• Bitbucket  
• GitLab  
• AWS Commit  
• Azure DevOps  

In INFO1111 you will use the University’s internal GitHub instance: https://github.sydney.edu.au.

### 6.3 A Command-Line Workflow

The following sequence mirrors the worked example shown in lecture:

```bash
# 1. Create a project and initialise the repository
echo "<html><body>Hello</body></html>" > paper.html
git init
git add paper.html
git commit -m "First paper"

# 2. Modify and commit again
echo "<style>body{color:red;}</style>" >> paper.html
git add paper.html
git commit -m "Added styling"

# 3. Further edits with inspection
git diff               # view unstaged changes
git add paper.html
git commit -m "Multicolour styling"

# 4. Experiment on a branch
git checkout -b exper <commit-hash-of-second-commit>
echo "<link rel='stylesheet' href='paper.css'>" >> paper.html
echo "body { color: blue; }" > paper.css
git add paper.*
git commit -m "Separate CSS"

# 5. Merge branch back into main line
git checkout master
git merge exper        # conflict resolution may be required
git add paper.html
git commit -m "Merged version"
git branch -d exper

# 6. Connect to remote and push
git remote add origin https://github.sydney.edu.au/INFO1111-2025/GitDemo.git
git push -u origin --all
```

A second collaborator would `git clone` the remote URL, make changes locally, commit, and `git push` to share their contributions. Git maintains a complete graph of commits, enabling both participants to pull, merge, and resolve conflicts without losing data.

### 6.4 Visualising Git Operations

The diagram below (Daniel Kinzler, GFDL) represents common Git commands:

```
commit --amend     merge
     |               |
   +----+        +----+
   |    |        |    |
   v    |        |    v
  A —— B —— C —— D —— E  (master)
          \
           F —— G        (feature)
```

Arrows depict parent relationships; branches label pointers to the latest commit in each development line.

### 6.5 Project Management Integration

GitHub and similar platforms embed issue trackers and **Kanban boards**. These boards visualise workflow stages such as *To Do*, *In Progress*, *Code Review*, and *Done*, aligning with Agile and Lean project-management principles and providing transparency for the entire team.

---

## 7 Independent Learning Focus: Race Conditions

A **race condition** occurs when the result of concurrent operations depends on their unpredictable timing:

• Two threads increment the same counter without mutual exclusion, producing an incorrect total.  
• A web application checks user credentials *after* starting a privileged action rather than *before*, enabling a malicious exploit.  

As software increasingly leverages parallel hardware and distributed services, detecting and preventing race conditions through locks, atomic operations, and careful protocol design becomes a critical professional competence.

---

## 8 Key Takeaways

1. Effective research requires both broad access to resources (library catalogues, scholarly databases, the open web) and systematic evaluation using the REVIEW criteria.  
2. Proper referencing underpins academic integrity; every external idea, quotation, or dataset must be acknowledged.  
3. Collaboration spans soft skills (teamwork and communication) and hard requirements (traceability, security, accountability).  
4. Version control solves the fundamental problem of multiple people editing the same artefact. Git, the leading distributed system, provides branching, merging, and a full historical record.  
5. Mastery of Git’s basic commands—`add`, `commit`, `branch`, `merge`, `push`, and `pull`—is indispensable for both academic projects and professional software development.  
6. Awareness of race conditions prepares you to design and implement safe concurrent systems.

These competencies collectively advance your professional toolkit, enabling you to source information responsibly, collaborate productively, and manage codebases confidently.

# Chapter 6  
INFO1111 — Computing 1A  
Week 06: Systems Thinking and Problem Solving

> “If I had an hour to solve a problem I’d spend 55 minutes thinking about the problem and 5 minutes thinking about solutions.”  
> — Albert Einstein  

> “The only way to know how a complex system will behave after you modify it is to modify it and see how it behaves.”  
> — George E. P. Box  

This chapter moves from last week’s focus on information management and version control to two complementary professional abilities:  
• Viewing software as part of interconnected, dynamic systems.  
• Tackling ill-structured problems methodically and creatively.

## 1 Linking Back to Week 05

The Week 05 independent learning theme, **race conditions**, demonstrated how event timing in multi-threaded programs can cause unpredictable outcomes. The classic illustration shows two people depositing money into a shared account:

Person 1: $200 + (add $100) = $300  
Person 2: $200 + (add $200) = $400  

Depending on the ordering of read-modify-write operations, the bank balance can end at $300, $400, or an erroneous duplicate update (“$400 ×”). Similar hazards appear in hardware and software alike.

The new self-learning concept for Week 06 is posed as a question: **What is Hill Climbing?** (It has nothing to do with hiking!) You are expected to investigate this optimisation strategy independently.

## 2 Systems and Emergent Behaviour

Software rarely exists in isolation. It operates within larger social, technical, and ecological environments known collectively as **systems**.

### 2.1 Reaction–Diffusion and Turing Patterns

Alan Turing’s reaction–diffusion model shows how simple chemical interactions can yield striking biological patterns such as stripes on zebras or spots on leopards. The Wikipedia entry “Reaction–diffusion system” illustrates these **Turing patterns**, reminding us that complex global structure can arise from local, repetitive rules.

### 2.2 Emergence

Emergent behaviours appear when the overall system exhibits properties not obvious from its individual components. Examples include:

• Ant colonies following basic foraging rules yet building intricate trail networks.  
• Chaotic fractals generated by deterministic equations.  
• The coordinated motion of fish schools and bird flocks (see the CGTN video linked in the slides).

### 2.3 Predictability and Design Questions

Key questions for computing professionals are:

1. Can we predict how a system will behave from the behaviour of its parts?  
2. Can we choose simple components that, when combined, deliver a desired global effect?

Answering these questions requires a mindset wider than algorithmic correctness; it needs **systems thinking**.

## 3 Systems Thinking

### 3.1 Definitions

At its broadest, systems thinking connects solutions, the systems that implement them, and the societies in which they operate. At a more focused level it examines components, their interactions, and their interrelationships to understand how they influence the whole.

### 3.2 Contrast with Traditional Analysis

Traditional systems analysis decomposes a problem into parts and studies each part in isolation. While indispensable in software engineering, that approach alone is incomplete. Systems thinking adds deliberate attention to the **structure** of relationships—how parts are connected and how those connections shape behaviour over time.

### 3.3 Characteristics of Complex Systems

• Interdependent components.  
• Dynamic feedback loops—balancing or reinforcing.  
• Ambiguous causality.  
• Emergent behaviour, sometimes producing uses the designers never envisaged.  
• Behaviour that changes as the system evolves, often unpredictably.

### 3.4 Why Systems Thinking Matters

1. **Unintended consequences**  
   • The *tragedy of the commons* in Borneo: spraying DDT killed mosquitoes (reducing malaria) but also poisoned parasites that restrained thatch-eating caterpillars, causing roofs to collapse.

2. **Complex system failures**  
   • *Therac-25* radiation machine: an interface task and a control task ran unsynchronised; rapid data entry created a race condition. The flaw escaped testing until operators became quick enough to trigger it.

3. **Hidden feedback cycles and delays**  
   What appears to be an isolated bug may stem from a long, reinforcing loop elsewhere. Recognising such cycles reveals leverage points for change.

Systems thinking therefore seeks to identify interdependencies, positive and negative feedback, and latency between cause and effect so that engineers can foresee risks and exploit opportunities.

## 4 Problem Solving

### 4.1 Illustrative Puzzles

The slides challenge you with several match-stick and pattern puzzles:

• Add only one matchstick to make “IX + IV = XI” true.  
• Re-arrange matchsticks so that an equals sign remains visible.  
• Move a single matchstick to turn a donkey while preserving its size and shape.  
• “Find the next number … PLEASE” — the answer is encoded by counting letters:  
  FIND (4), THE (3), NEXT (4), NUMBER (6), PLEASE (6).

A bigger exercise shows seven directed matchsticks forming a pathway. By reversing just two arrows it becomes possible to traverse every stick exactly once.

These puzzles highlight two complementary lessons:

1. Sometimes the *solution* is tricky even when the *problem* is obvious.  
2. Sometimes the *problem statement* itself is the tricky part.

### 4.2 Understanding the Problem

In professional settings, “understanding the problem” equates to:

• Precise problem specifications.  
• Clear requirement documents.  
• Explicit constraints.

Poor requirements may be incorrect, incomplete, vague, conflicting, missing entirely, or prematurely solution-focused. Compare:

• “The system shall have good usability.” (vague)  
• “The system shall respond very fast.” (vague)  
• “The system shall respond in less than two seconds.” (measurable)  
• “The system shall be error-free.” (impractical absolute)

### 4.3 The Elevator Example

A hotel added extra floors and new elevators, prompting complaints that the elevators were too slow. Engineering options—faster motors or additional shafts—were prohibitively expensive. A simple experiment installed full-length mirrors beside the elevators; guests, now occupied, stopped complaining. The underlying issue was boredom, not elevator speed. The story underscores the need to identify the *real* problem before designing a fix.

### 4.4 Structured Approaches

The slides reference several problem-solving frameworks (Six Thinking Hats, design-thinking cycles, and others depicted graphically). Common themes include:

• **Brainstorming** phases: forming, *storming*, norming, performing.  
• Techniques: word association, mind mapping, word banks, visual association, “what if?”, exaggeration, prepositional thinking, “why?” chains, role storming, super-power scenarios, SWOT analysis, questioning assumptions.  
• **Decomposition** (divide and conquer): functional, procedural, domain, or role-based splits.  
  – Minimise **coupling** (dependencies between components).  
  – Maximise **cohesion** (focused responsibility inside each component).  
• Creativity practice: “In 30 seconds, list as many uses as you can for 50 paper clips.”

### 4.5 Value of Expert Problem Solvers

Indicative Australian salaries (slide sources: PayScale and Glassdoor) show increasing remuneration with roles demanding broader analytical insight:

Software Tester ≈ $62 k  
Programmer ≈ $69 k  
Software Engineer ≈ $75 k  
Business Analyst ≈ $82 k  
IT Project Manager ≈ $103–128 k  

The market rewards the ability to understand complex requirements, foresee system-level impacts, and guide projects accordingly.

### 4.6 Levels and Languages of Software Solutions

A diagram in the lecture contrasts **system-level architecture** with **unit-level algorithms**. Both levels matter: architecture shapes long-term scalability and reliability; algorithms deliver the day-to-day functionality users experience.

## 5 Independent Learning Prompt: Hill Climbing

The Week 06 self-learning exercise asks simply, “What is Hill Climbing? (nothing to do with hiking!)”. Investigate this optimisation heuristic and be prepared to relate it to search strategies and local optima in future discussions.

## 6 Key Points to Remember

1. Systems exhibit emergent behaviour; small, simple rules can produce rich, unpredictable outcomes.  
2. Systems thinking supplements traditional decomposition by focusing on relationships, feedback loops, and delays.  
3. Unintended consequences—such as DDT-induced roof collapses or race conditions in Therac-25—underline the need for a systemic perspective.  
4. Effective problem solving begins with an accurate understanding of the real problem, not just the obvious symptom.  
5. Structured creativity techniques, clear requirements, and thoughtful decomposition minimise coupling, maximise cohesion, and lead to robust solutions.  
6. The professional world values—and pays for—the capacity to analyse complex systems and articulate precise, impactful solutions.

With these principles, you are better prepared to design software that operates reliably within the intricate systems of the real world and to tackle the open-ended problems that inevitably arise.

# Chapter 7  
INFO1111 — Computing 1A  
Week 06 B: Systems Thinking and Complexity

## 1 From Decomposition to Complexity

Last week’s discussion introduced **systems thinking** as a complement to classical problem decomposition. This chapter deepens that perspective by:

1. Revisiting the Work Breakdown Structure (WBS) used in project planning.  
2. Examining principles that govern complex, evolving systems.  
3. Exploring pattern formation, panic models, and other illustrative simulations.  
4. Introducing design architectures, modularity, and tools for analysing dependence.  
5. Summarising core tenets that underpin the systems‐thinking approach.

---

## 2 Work Breakdown Structures in Software Projects

### 2.1 Definition and Purpose

A **Work Breakdown Structure (WBS)** is a deliverable-oriented hierarchy that captures *all* work required to complete a project. It:

• Defines the total project scope.  
• Provides a firm basis for schedule, cost, resource, and change management.  
• Relies on **decomposition**—iteratively subdividing high-level deliverables into smaller, more manageable components.

### 2.2 Example: Web-Development WBS

A typical web-development WBS may be organised around major product features (e.g. user authentication, content management, payment gateway), service layers, or deployment environments. Each branch is decomposed until every leaf node represents a work package that can be scheduled, costed, and assigned.

### 2.3 Four Approaches to Building a WBS

1. **Analogy** Inspect WBSs from similar projects and tailor them.  
2. **Top-down** List the largest deliverables first, then break them down.  
3. **Bottom-up** Brainstorm specific tasks, then aggregate upward.  
4. **Mind-mapping** Capture tasks in a non-linear, branching diagram before converting the result into a structured hierarchy.

---

## 3 Complex Systems Concepts for Systems Design

### 3.1 Design Architectures

Large, complex, and continually evolving designs are now routine. A **design architecture** is:

• A description of all entities in a system and the relationships between them.  
• A means of assigning work across teams or organisations.  
• A strategic asset: the architecture chosen determines how value can be created, captured, and extended over time.  
  (Carliss Y. Baldwin, 2006)

### 3.2 The Design Precedence Matrix

Borrowed from automotive engineering, the **design precedence matrix** represents dependencies among design tasks. Each cell records whether task *i* must precede task *j*. When visualised, tightly coupled clusters become apparent, guiding sequencing, parallelisation, and risk management.

---

## 4 Systems Thinking in Action

### 4.1 Feedback and Delay: The Pesticide Example

Daniel Aronson’s pesticide case demonstrates counter-intuitive outcomes:

1. **Action** Apply pesticide to crops.  
2. **Short-term effect** Fewer insects → higher yield.  
3. **Long-term effect** Natural predators are also reduced; pest populations rebound, often stronger than before.  
4. **Result** Farmers apply more pesticide, reinforcing the vicious cycle.

### 4.2 Predator-and-Prey Simulation

A simple on-screen simulation (demonstrated in lecture) shows oscillating populations arising from two local rules: predators starve without prey; prey reproduce when predators are scarce. This illustrates how non-linear feedback can generate sustained, dynamic behaviour.

### 4.3 Parts, Wholes, and Relationships

A diagram labelled “Fundamental: Parts, Wholes and Relationships” emphasises:

• Internal properties of each part may remain unchanged.  
• System-level properties can shift dramatically when relationships are altered.  
• Some components are strongly affected by changes elsewhere; others are weakly affected.

---

## 5 Key Systems Principles (Simplified)

1. **Openness** A system’s behaviour must be analysed in the context of its environment. Define the boundary, then distinguish controllable and uncontrollable variables.  
2. **Purposefulness** Actors are value-guided; understanding *why* they behave as they do is essential.  
   • Reaction → response → action indicates an active role of choice.  
3. **Adaptiveness** Systems adjust their structure or behaviour in response to feedback.  
4. **Emergent Properties** Attributes of the whole cannot be deduced solely from the parts; they arise from complex interactions.  
5. **Multidimensionality** Seemingly opposing tendencies may coexist and complement one another; multiple dimensions interact simultaneously.  
6. **Counter-intuitiveness** Well-intended interventions can yield opposite results.  
7. **Inflection Points** Thresholds beyond which small changes produce disproportionate effects.

---

## 6 Systems Theory and the Systems Approach

• **Systems theory** is a transdisciplinary study of the abstract organisation of complex phenomena, independent of substance, scale, or timeframe.  
• The **systems approach** analyses the total system, its goals, individual components, and the interdependencies between them.

---

## 7 Pattern Formation and Local Interaction Models

### 7.1 Binary-Grid Exercise

A classroom exercise used a 16 × 16 grid of 0 s and 1 s to illustrate how simple local rules can generate global patterns. Starting from different initial configurations results in distinct, sometimes unexpected, macro-structures—even though there are 2ⁿ possible initial states.

### 7.2 Model of Panic

In a *panic* model:

• Each person’s emotional state (calm ↔ panicky) depends only on their immediate neighbours.  
• Lighter shades in the diagram denote calm individuals; darker shades denote panic.  
• Two nearly identical local contexts can result in opposite outcomes: panic or calm, highlighting sensitivity to initial conditions.

### 7.3 Crowded-Auditorium Simulation

The lecture slides show multiple iterations (“N repetitions”) of the panic rule applied to a theatre-sized crowd. Step counts record the changing number of calm (0) and panicky (1) agents:

Step 0 0 s = 526, 1 s = 498  
Step 1 0 s = 557, 1 s = 467  
Step 2 0 s = 584, 1 s = 440  
…  
Step 8 0 s = 614, 1 s = 410  

The trend illustrates how collective behaviour can stabilise even when individual transitions remain stochastic.

---

## 8 Systems Design Thinking and Modularity

### 8.1 Concept of Modularity

A design is **modular** when:

• Components (modules) are highly interdependent *within* themselves but nearly independent *between* each other.  
• Modularity is an architectural choice, not an intrinsic property; architects decide where to place boundaries.

### 8.2 IBM System/360: Proof of Modularity

Developed between 1962 and 1967, IBM’s System/360 introduced the first modular computer design:

• Standardised **SLT (Solid Logic Technology)** circuits, cards, and boards.  
• Multiple processors (Endicott, Hursley, Poughkeepsie) and memory components managed by dedicated corporate groups.  
• Unified I/O control structures and a family of compilers (FORTRAN, COBOL).  
• Thirty-five distinct modules were planned—from “SLT architecture” (Module 1) to “Shipment, Delivery and Installation” (Module 35).  
• The hardware and application software were modular; the system software resisted full modularisation, highlighting limits to the approach.

### 8.3 Dependence Structure Matrix (DSM)

A **DSM** lists components on both axes and marks dependency relationships. Clusters on the diagonal reveal tightly coupled subsystems. The matrix characterises the *landscape* that designers must traverse when making changes.

### 8.4 Measuring Modularity in Software

Using DSM tools, researchers compared two code bases of similar size:

• **Mozilla (early open-source release)**  
  – Coordination cost: 30,537,703  
  – Change cost: 17.35 %  

• **Linux (contemporary kernel snapshot)**  
  – Coordination cost: 15,814,993  
  – Change cost: 6.65 %  

Lower costs in Linux indicate a more modular structure, reducing effort to coordinate changes across files.

---

## 9 Fundamental Tenets of the Systems-Thinking Approach

(Source: Wikipedia entry on Systems Thinking)

1. **Interdependence** Isolated elements cannot form a system.  
2. **Holism** Emergent properties require holistic examination.  
3. **Goal Seeking** Interactions drive the system toward explicit or implicit objectives.  
4. **Inputs & Outputs** Closed systems have fixed inputs; open systems accept new inputs from the environment.  
5. **Transformation** Inputs are converted into outputs to achieve goals.  
6. **Entropy** Disorder increases unless energy or information is added.  
7. **Regulation** Feedback mechanisms enable predictable operation.  
8. **Hierarchy** Complex wholes comprise nested subsystems.  
9. **Differentiation** Specialised units perform specialised functions.  
10. **Equifinality** Different paths can achieve the same objective.  
11. **Multifinality** The same inputs can yield multiple outcomes.

---

## 10 Closing Remarks

Project planning tools such as the WBS remind us to *break work down*, yet complex behaviour often emerges *between* the pieces once they interact. Design architectures, modularity, dependence matrices, and the principles outlined above equip computing professionals to navigate this duality—structuring large projects while anticipating the non-linear dynamics that arise in real-world systems.

# Chapter 8  
INFO1111 — Computing 1A  
Week 07 A: Intellectual Property and Professional Ethics

---

## 1 Framing the Discussion

Shepard Fairey reminds us that copyright exists to *encourage* creativity, not suppress it; Bill Gates notes that intellectual property “has the shelf-life of a banana.”  Ben Carson adds that every technical advance must be “tempered with ethics.”  
This chapter therefore considers two tightly coupled themes:

1. Intellectual property (IP) — the legal structures that govern ownership and exploitation of ideas in computing.  
2. Professional ethics — the frameworks that help practitioners choose, justify, and defend right action when technology intersects with society.

A brief self-learning pointer for the week is also included: the **Halting Problem** and its significance.

---

## Part I Intellectual Property

### 2 What Counts as Intellectual Property?

In computing, intellectual property is “the ownership of ideas and control over the tangible or virtual representations of those ideas.”  Ownership entails the right to benefit from, and the right to decide about, subsequent use.  Control mechanisms differ, but all rest on the premise that ideas can acquire economic value once expressed.

### 3 Historical Foundations

• 14th century — Letters patent granted public, documented rights.  
• 1624 — Statute of Monopolies curbed abuse of such grants, limiting patents to genuinely novel inventions.  
• 1710 — The Statute of Anne (Copyright Act) shifted control of literary works from printers’ guilds to authors for 14 → 28 years, explicitly “for the encouragement of learning.”

These statutes created the modern distinction between *ideas* and the *exclusive rights* temporarily granted to stimulate further creativity.

### 4 Who Owns What?  Practical Questions

1. Video of a street performance: ownership may rest with the filmmaker, the commissioner, the performers, or some combination—Australian law treats films, photographs, portraits, and commissioned works differently.  
2. Commissioned software:  
   • If an employee writes the code “in the course of employment,” the employer owns the copyright.  
   • Otherwise, ownership remains with the developer unless a contract assigns it elsewhere.  
   • University coursework may be governed by specific institutional agreements; always check.

### 5 Mechanisms for Protecting Ideas

1. **Trade secrets**  
   Hide the implementation.  Protection endures only while secrecy is maintained—inventory control, NDAs, limited distribution of executables, and physical or electronic security all matter.

2. **Copyright**  
   Automatically vests (in Australia and France) once a work exists; in the US and Canada it must be “fixed in a tangible medium.”  The owner alone may reproduce, publish, adapt, or communicate the software, subject to exceptions:

   • Fair dealing—research, criticism, parody, satire.  
   • Flexible dealing—teaching.  
   • Classroom demonstration and limited back-up copies.

   Application Programming Interfaces (APIs) sit at the edge of copyright: the Oracle v Google litigation over Java APIs illustrates ongoing uncertainty.

3. **Patents**  
   Grant a time-limited monopoly over a *technical* solution that is new, inventive, and useful.  Algorithms in the abstract are non-patentable, but software-implemented inventions sometimes qualify.  Filing requires full public disclosure to IP Australia (or other patent offices).

4. **Trademarks**  
   Protect distinctive brands—words, logos, sounds, even smells—provided they are actively used.  Registration is not compulsory but greatly strengthens enforcement.

### 6 Licensing: Enabling Use While Retaining Rights

After ownership is established, a licence determines the rights granted to others:

• Proprietary End-User Licence Agreements (EULAs) typically declare that software is “licensed, not sold.”  
• Free and Open Source Software (FOSS) licences range from permissive (BSD) to reciprocal (GNU GPL), each defining use, copying, modification, distribution, sub-licensing, and liability.

### 7 Commercialisation of Software IP

#### 7.1 An Illustrative Exercise

A student proposes an app that diagnoses car suspension problems via phone accelerometers.  A friend builds a prototype under academic supervision, then outsources commercial development.  The system “goes viral,” and a motoring organisation offers AU$12 million for the technology.  

Questions to resolve:

• Who controls the underlying IP (ideas, code, data)?  
• How is AU$12 million divided among originator, prototype developer, supervisor, freelancer, and any third-party funders or hosts?

#### 7.2 The Language of Start-up Finance

Key terms encountered in commercialisation include:

IPO, angel investor, seed funding, incubator, accelerator, value proposition, business model, due diligence, NDA, licensing, technology transfer, crowd-sourcing, R & D tax incentives.

#### 7.3 Common Business Models in the Digital Economy

(Adapted from digitalenterprise.org)

• Brokerage, marketplace, auction, transaction broker, virtual marketplace  
• Advertising, portal, classifieds, user-registration, infomediary, advertising networks, audience measurement, incentive marketing  
• Merchant, manufacturer, direct sales, affiliate, pay-per-click, revenue-sharing  
• Community, open content, social networking, content services, networking, utility, metered usage, metered subscription  
• Virtual, click-and-mortar, bit vendor

---

## 8 Case Studies and Scenarios in IP

1. **Kim Dotcom and MegaUpload**  
   Indicted for racketeering and large-scale copyright infringement.  Raises the question: how far must a developer go to prevent illegal use of their platform?

2. **Reverse-engineering a proprietary algorithm**  
   A start-up gains access to protected code, studies it, then rewrites a functionally identical component.  Developers must judge whether such conduct breaches copyright, trade-secret obligations, or software-licence terms.

3. **Open Source vs Proprietary Release**  
   Bill Gates’ 1976 “Open Letter to Hobbyists” and the later Creative Commons movement illustrate contrasting philosophies: *free beer* (no cost) versus *free speech* (no constraints).

---

## Part II Professional Ethics

### 9 Why Ethics Matters to Computing Professionals

Professionals confront questions such as:

• Do I always know right from wrong?  
• How will my choices affect users, employers, the public, or unseen stakeholders?  
• Is workplace judgment different from personal morality?  
• What if another professional’s “right” is my “wrong”?

### 10 Ethics, Morals, and Theory

• **Morals** are internal principles guiding personal behaviour.  
• **Ethics** are externally accepted rules within a professional or social context.

Frameworks for ethical reasoning:

1. **Teleological ethics** (consequence-based)  
   • Egoism — value determined by personal outcome, tempered by likely reactions of others.  
   • Utilitarianism — “the greatest good for the greatest number,” weighed across all affected.

2. **Deontological ethics** (duty-based)  
   W. D. Ross lists prima facie duties: fidelity, reparation, gratitude, justice, beneficence, non-maleficence, self-improvement.  Conflicts between duties demand careful prioritisation.

3. **Contractarianism**  
   Ethical action flows from implicit or explicit agreements among rational agents.

### 11 Professional Codes

1. **Australian Computer Society (ACS) — Code of Professional Conduct**  
   Primacy of public interest, enhancement of quality of life, honesty, competence, professional development, professionalism.

2. **ACM/IEEE-CS — Code of Ethics (2018 revision)**  
   • Contribute to society and human well-being.  
   • Avoid harm.  
   • Be honest and trustworthy.  
   • Be fair; avoid discrimination.  
   • Respect intellectual effort.  
   • Respect privacy and confidentiality.  
   • Perform only within areas of competence.  
   • Design robustly secure and socially beneficial systems.

The bodies can investigate complaints and impose penalties.

### 12 Corporate Ethics and Social Responsibility

• Would you work for a tobacco company if offered a higher salary?  
• “Fourth bottom line” thinking extends profit, people, and planet with purpose or principles.  Ethical stance increasingly influences employment and consumer choice.

### 13 Ethics in Action: Case Studies

1. **Project Over-billing**  
   A consultancy conceals a quicker, cheaper solution to preserve a AU$6 million contract.  Teleological analysis weighs staff livelihoods against client deception; deontological analysis highlights duties of honesty and competence.

2. **Data-Mining and Manipulation**  
   • Facebook mood-enhancement experiment manipulated user news-feeds to study “emotional contagion.”  
   • Cambridge Analytica utilised psychometric profiling for political advertising.  Issues include informed consent, transparency, and potential social harm.

3. **Privacy**

   • *Toysmart v FTC*: a bankrupt e-retailer attempted to sell customer data despite a non-sharing privacy policy.  
   • *Google Street View War-Driving*: collection of Wi-Fi SSIDs, MAC addresses, and plaintext payloads worldwide.  Google initially blamed a “rogue engineer,” later shown to have commissioned the program.

   Administrators must decide when to override confidentiality—for example, a principal requesting access to a teacher’s computer or an employer asking to covertly track employee phones.

4. **Liability for Unreliable Software**

   • *Therac-25*: radiation overdoses attributed to inadequate software design, lack of independent review, and poor error handling.  
   • *Left-pad cascade*: a tiny, open-source padding function propagates through layered dependencies; a hidden bug crashes critical systems.  Questions of negligence, strict liability, and professional malpractice arise.

5. **Unauthorised Access**

   • Garry McKinnon’s military network hack (“Your security is crap”) challenges the boundary between ethical hacking and criminal intrusion.  
   • Student testers with production data must still respect privacy constraints; removing names does not always anonymise data.

6. **Intentional Wrongdoing**

   • Volkswagen diesel-emissions “defeat device” demonstrates deliberate deception via software.  
   • StuxNet and the Morris Worm show nation-state and accidental impacts of code released into the wild.

### 14 Practising Within One’s Abilities

ACM Code clause 2.2 requires professionals to “acquire and maintain competence.”  Self-assessment, peer review, and honest communication of limitations reduce risk of harm.  Over-promising or misrepresenting capability can constitute negligent misrepresentation or fraud.

---

## 15 Self-Learning Focus: The Halting Problem

The **Halting Problem** asks whether a general algorithm can determine, for every possible program and input, whether the program eventually halts.  Alan Turing proved in 1936 that no such algorithm exists.  Significance:

• Establishes inherent limits to computation.  
• Underpins undecidable questions in software verification, security analysis, and automated reasoning.  
• Provides context for why some vulnerabilities or bugs can never be algorithmically detected in advance.

---

## 16 Key Take-aways

1. Intellectual property law provides multiple, overlapping tools—trade secrets, copyright, patents, trademarks—and the choice among them influences how software can be exploited or shared.  
2. Licensing translates ownership into controlled use; understanding EULAs and FOSS variants is essential professional knowledge.  
3. Ethical reasoning relies on clear frameworks (utilitarian, deontological, professional codes) and on awareness of real-world cases that test those frameworks.  
4. Liability and responsibility extend far beyond writing “working code”: they encompass foreseeability of misuse, adequacy of testing, maintenance of competence, and honesty in representation.  
5. Some problems—like the Halting Problem—are provably unsolvable, reminding us that technical limits coexist with legal and ethical constraints.

The modern computing professional must navigate all three dimensions—legal rights, ethical duties, and technical limits—while designing systems that serve society’s evolving needs.

# Chapter 9  
INFO1111 — Computing 1A  
Week 07 B: Law, Morality, and Professional Responsibility  

---

## 1 What Ethics Is—and Is Not  

Ethics is a system of moral principles.  It sets out the principles of right and wrong and the principles of conduct that govern the behaviour of an individual or a group.  Whereas personal morals tend to be private and internal, ethics operates in the social or professional sphere, providing shared rules against which conduct can be judged.

## 2 Disentangling Ethics, Law, and Morality  

Because the terms are often conflated, careful distinctions are useful:

• Ethics vs Morality.  Morality refers to the personal convictions you hold about right and wrong; ethics refers to the publicly acknowledged standards that guide professional or communal behaviour.  

• Legal vs Ethical.  Law codifies minimum standards of behaviour enforceable by the state; ethics may demand a higher standard than the law requires.  An act can therefore be legal yet unethical, or illegal yet ethically defensible.

### 2.1 Four Logical Categories  

Considering both legality and morality yields four possible combinations:

1. Legal and Moral — e.g. designing a system to be safe.  
2. Legal and Immoral — e.g. displaying an Aboriginal person in a human zoo in 1940s Australia.  
3. Illegal and Moral — e.g. parking in a no-parking zone to help an injured person.  
4. Illegal and Immoral — e.g. killing an innocent person.

The examples show that legality offers no guarantee of moral worth, and immorality is not always unlawful.

## 3 Analysing an Ethical Scenario  

Professional decisions rarely come as multiple-choice questions.  A systematic approach helps:

1. Read and understand the scenario in full.  
2. Identify the ethical dilemma — who is affected and what conflicting values are at stake?  
3. Reach a decision (yes, no, or neutral).  
4. State the reasoning behind that decision in your own words.  
5. Relate the reasoning to at least one recognised ethical framework (utilitarianism, duty-based ethics, codes of professional conduct, etc.).

The quality of the analysis rests on the clarity with which you identify issues, apply frameworks, and justify conclusions, not on whether the final decision is “right” in some absolute sense.

## 4 Case Study: When Open Source Goes Wrong  

A developer writes a game app as a hobby project and releases the code as open source.  A third party incorporates the freely available code into a traffic-control system.  A latent bug causes system failure, leading to car crashes and three fatalities.  The question is: does the original developer bear any ethical responsibility?

Key points for analysis:

• Foreseeability.  Could a reasonable person anticipate that the code might be reused in safety-critical contexts?  
• Duty of care.  Does distributing code to the public create a duty to test it to a particular standard?  
• Attribution of responsibility.  How much responsibility shifts to the party that selected and deployed the code in a high-risk system?  
• Applicable professional codes.  Both ACM and ACS codes require professionals to avoid harm and to act with competence and diligence; these clauses inform the evaluation of responsibility.

Different ethical frameworks may yield different conclusions, but each must grapple with the same facts.

## 5 Professional Codes of Ethics  

Computing professionals are not left to improvise standards:

• Association for Computing Machinery (ACM) — Code of Ethics and Professional Conduct.  
• Australian Computer Society (ACS) — Code of Professional Conduct and Professional Practice.

Both codes articulate duties such as contributing to society and human well-being, avoiding harm, acting honestly, and maintaining professional competence.  They form a concrete benchmark against which ethical analyses and professional behaviour can be measured.

---

Ethical practice in computing therefore demands more than knowledge of statutes.  It requires a disciplined habit of recognising dilemmas, analysing them through established frameworks, and aligning decisions with both personal conscience and professional codes.

